---
layout: tutorial
comments: true
title: Introdução às Redes Neurais Artificiais
subtitle: "Uma apresentação teórica e intuitiva às redes neurais artificiais."
date: 2017-03-05
true-dt: 2017-04-14
tags: [Tutorial]
author: "Matheus Facure"
header-img: "img/dark-ann.jpg"
---


<p>Neste tutorial nós não vamos implementar nada e nenhum código será utilizado. Vou apenas explicar o que são as redes neurais artificiais, primeiro de maneira intuitiva e visual e depois com o modelo matemático. A minha intenção é desmistificar as redes neurais artificiais (RNA) como um método complexo de Aprendizado de Máquina.</p>

<h2>Conteúdo</h2>
<ul>
	<li><a href="#Requisitos">Pré-Requisitos</a></li>
	<li><a href="#Neuronios">Neurônios</a></li>
	<li><a href="#Redes-Neurais">Redes Neurais</a>
<ul>
	<li><a href="#Intuição">Intuição</a></li>
	<li><a href="#Modelo">Modelo Matemático</a></li>
</ul>
</li>
	<li><a href="#Deep-Learning">Deep Learning e Aprendizado de Representações</a></li>
	<li><a href="#Finais">Considerações Finais</a></li>
	<li><a href="#Referências">Referências</a></li>
</ul>
<h2 id="Requisitos">Pré-Requisitos</h2>
<p>Vou pressupor que você tenha os conhecimentos especificados no tutorial sobre <a href="https://matheusfacure.github.io/2017/01/15/pre-req-ml/">matemática e programação para aprendizado de máquina</a>, isto é, que sabe cálculo (derivadas), o básico de álgebra linear, de estatística e de programação. Eu também vou pressupor que você viu os tutoriais anteriores a esse. <a href="https://matheusfacure.github.io/tutorials/">Meus tutoriais </a> são ordenados de maneira lógica e sugiro fortemente que você se atenha à ordem deles para maior compreensão.</p>

<h2 id="Neuronios">Neurônios</h2>
<p>Um neurônio de uma rede neural é um componente que calcula a soma ponderada de vários <em>inputs,</em> aplica uma função e passa o resultado adiante:</p>

<img class="img-responsive center-block thumbnail" src="/img/tutorial/perceptron.png" alt="perceptron"/>

<p>Você pode estar se lembrando dessa imagem do <a href="https://matheusfacure.github.io/2017/02/25/regr-log/#indoalem">tutorial de regressão logística</a>, quando falei dos <em>perceptrons</em>. Eu usei a mesma imagem para ressaltar que <strong>tanto regressão logística quando <em>perceptrons</em> são tipos de neurônios</strong>. É por isso que muitos chamam as redes neurais de <em>multilayer perceptron.</em> No caso da regressão logística, estamos tratando de um neurônio com uma função sigmóide após a soma ponderada. Além da regressão logística, <strong>a regressão linear também é um tipo de neurônio</strong>, só que a função após a soma ponderada é a identidade \( f(x)=x\). De forma geral, um neurônio é composto por uma transformação linear, seguida de alguma função:</p>

$$f(\pmb{X} \pmb{w})$$

<p>Em que \( \pmb{X}\) é a matriz de dados e \( \pmb{w}\) é o vetor de parâmetros.</p>

<h2 id="Redes-Neurais">Redes Neurais</h2>
<h3 id="Intuição">Intuição</h3>
<p>Quando utilizamos vários neurônios em paralelo temos uma rede neural. Nós podemos pensar em cada neurônio como recebendo sinais das variáveis dos <em>inputs</em> e passando adiante uma versão ponderada e tratada desse sinal. Esses <strong>neurônios em paralelo formam uma camada oculta da rede neural</strong>. Nós podemos tratar o <em>output</em> de cada neurônio como uma variável do <em>input </em>de uma outra camada oculta. Assim, podemos empilhar camadas ocultas e produzir uma rede neural profunda:</p>

<img class="img-responsive center-block thumbnail" src="/img/tutorial/rede_grafo.jpg" alt="rede_grafo" />

<h3 id="Modelo">Modelo Matemático</h3>
<p>Eu particularmente acho a imagem acima extremamente confusa e prefiro pensar nas redes neurais de outra forma: como <strong>aninhamentos sucessivos de diversas transformações lineares seguidas por alguma função diferenciável, que é aplicada elemento a elemento da matriz de entrada</strong>. Para melhor entendê-las, vamos partir de uma rede neural bem simples: um modelo de regressão linear, que pode ser entendido como uma rede neural com um único neurônio:</p>

$$ \pmb{X} \pmb{w} = \pmb{y} $$

$$
\begin{bmatrix}
1 & x_{11} & ... & x_{1d} \\
1 & x_{21} & ... & x_{2d} \\
\vdots & \vdots& \vdots & \vdots \\
1 & x_{n1} & ... & x_{nd} \\
\end{bmatrix}
\times
\begin{bmatrix}
w_0 \\
w_1 \\
\vdots \\
w_d \\
\end{bmatrix}
=
\begin{bmatrix}
y_0 \\
y_1 \\
\vdots \\
y_n \\
\end{bmatrix}
$$

<p>(Por simplicidade de notação, vamos omitir \( \pmb{\epsilon}\)). Para adicionar mais neurônios nessa rede neural, basta então expandir a matriz de parâmetros. Além disso, vamos multiplicar a multiplicação de matrizes por mais um vetor, mantendo a consistência do <em>output.</em> Temos assim o modelo de uma rede neural com mais neurônios:</p>

$$ (\pmb{X} \pmb{W_1})\pmb{w} = \pmb{y} $$

$$
\begin{bmatrix}
1 & x_{11} & ... & x_{1d} \\
1 & x_{21} & ... & x_{2d} \\
\vdots & \vdots& \vdots & \vdots \\
1 & x_{n1} & ... & x_{nd} \\
\end{bmatrix}
\times
\begin{bmatrix}
w_{01} & w_{01} & ... & w_{0m} \\
w_{11} & w_{11} & ... & w_{1m} \\
\vdots & \vdots& \vdots & \vdots \\
w_{d1} & w_{d1} & ... & w_{dm} \\
\end{bmatrix}
\times
\begin{bmatrix}
w_{01} \\
w_{11}  \\
\vdots \\
w_{d1}  \\
\end{bmatrix}
=
\begin{bmatrix}
y_0 \\
y_1 \\
\vdots \\
y_n \\
\end{bmatrix}
$$

<p>É importante perceber que a matriz \( \pmb{W}\) é a camada oculta da rede neural e cada coluna dessa matriz é um neurônio da camada oculta. Nós podemos pensar no vetor \( \pmb{w}\) como uma camada de saída com um único neurônio, que recebe o sinal dos neurônios anteriores, pondera-os e produz o <em>output </em>final da rede.</p>
<p>A rede neural acima não é muito interessante do ponto de vista prático pois só consegue representar funções lineares. Felizmente, podemos arrumar isso facilmente, alterando o modelo da seguinte forma:</p>

$$ \phi(\pmb{X} \pmb{W_1})\pmb{w} = \pmb{y} $$

<p>Em que \( \phi\) é alguma <strong>função não linear diferenciável</strong>. Ela precisa ser diferenciável pois vamos treinar a rede neural com gradiente descendente, assim como fizemos num <a href="https://matheusfacure.github.io/2017/02/20/MQO-Gradiente-Descendente/">tutorial passado</a>. O tipo mais comum de função não linear que utilizamos é a <strong>unidade linear retificada</strong>, ou <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a>:</p>

<img class="img-responsive center-block thumbnail" src="/img/tutorial/relu.png" alt="relu"/>

<p>Formalmente, a ReLU é definida como \( f(x) = \max(0,x)\). Essa função tem propriedades muito interessantes, como ser parcialmente linear, o que facilita na hora do treinamento, e ter derivadas muito simples: \( 0\), se \( x < 0\) e \( 1\), se \( x > 0\). (Na prática, o ponto onde a derivada não está definida é implementado como fazendo parte de alguma das regiões onde ela é bem comportada).</p>
<p>Quando colocamos a não linearidade, a <strong>RNA consegue representar qualquer função</strong>, dado um número suficiente de neurônios. Quanto maior o número de neurônios, maior a capacidade do modelo. É importante ressaltar também que, quando introduzimos a não linearidade na rede neural, a <strong>função custo que otimizaremos se torna não convexa </strong>e extremamente complicada de otimizar, dificultando consideravelmente o processo de treinamento.</p>
<p>No modelo de RNA acima, nós só utilizamos uma camada oculta, mas nada impede que utilizemos um número maior. Por exemplo, podemos construir uma rede neural artificial com duas camadas ocultas da seguinte forma:</p>

$$ \phi(\phi(\pmb{X} \pmb{W_1})\pmb{W_2}) \pmb{w} = \pmb{y} $$

<h2 id="Deep-Learning">Deep Learning e Aprendizado de Representações</h2>
<p>Em Aprendizado de Máquina clássico, um problema que sempre aparece é que a</p>

<figure class="figure pull-right thumbnail" style="width: 40%;">
  <img src="/img/tutorial/ml_classic_deep.png" class="img-responsive center-block" alt="">
  <figcaption class="figure-caption text-center">Adaptada do livro Deep Learning, Goodfellow et al, 2016. Células cinzas marcam as etapas automatizadas do processo de análise.</figcaption>
</figure>

<p>tarefa mais difícil não é treinar a máquina, mas sim engenhar variáveis que auxiliem no aprendizado. Em reconhecimento de imagem, um exemplo de como isso acontece pode ser visto nos inúmeros e nada simples pré-processamentos que a imagem passa antes de ser alimentada a um algoritmo de Aprendizado de Máquina: filtros de ruído, segmentação, aumento de contraste, detecção de contornos, etc. As redes neurais artificiais (RNAs) surgem como forma de resolver esse problema: em vez de necessitarem de alguém para criar variáveis representativas manualmente, as redes neurais são capazes de aprendê-las sozinhas. Em distinção ao Aprendizado de Máquina clássico (imagem ao lado, coluna esquerda), as redes neurais são comumente utilizadas em um novo tipo de Aprendizado de Máquina, que leva o nome de <strong>aprendizado de representações ou <em>Deep Learning</em></strong> (imagem ao lado, coluna direita), no qual <strong>além de aprender um mapeamento entre características representativas e um <em>output</em> desejado, a máquina consegue aprender as próprias características representativas de maneira automática</strong>.</p>
<p>As técnicas de <em>Deep Learning </em>se baseiam principalmente na utilização de redes neurais profundas. Podemos pensar nas <strong>diversas camadas ocultas de uma rede neural profunda como aprendendo níveis de abstrações hierárquicos</strong>. Em reconhecimento de imagens, por exemplo, podemos pensar nas camadas mais baixas (próximas aos inputs) como aprendendo a detectar traços e variação de luminosidade, enquanto que as camadas superiores aprendem a juntar esses traços em partes de objetos. Essas partes então podem ser utilizadas por um modelo linear para discriminar entre um ou outro objetos.</p>
<p>No modelo matemático, podemos dizer que a parte \( \phi(\phi(\pmb{X} \pmb{W_1})\pmb{W_2})\) do modelo (ou seja, as camadas ocultas), são responsáveis por <strong>aprender variáveis representativas</strong> que ajudarão na tarefa de Aprendizado de Máquina. Nós podemos abstrair essa matrix para \( \pmb{X^*} = \phi(\phi(\pmb{X} \pmb{W_1})\pmb{W_2})\),  em que \( \pmb{X^*}\) é a matriz de dados com as variáveis representativas. Em seguida, simplesmente utilizamos essas representações em conjunto com algum modelo linear:</p>

$$ \pmb{X^*} \pmb{w} = \pmb{y} $$

<h2 id="Finais">Considerações Finais</h2>
<p>Os modelos baseados em redes neurais artificiais são os que mais ganharam atenção nos últimos anos por conseguirem resolver problemas de IA nos quais se conseguia pouco avanço com outras técnicas. Nessa introdução às redes neurais, só falei sobre o modelo mais comum, mas saiba que as redes neurais fornecem uma flexibilidade absurda quanto a arquitetura dos modelos. Isso abre muito espaço para vários desenvolvimentos criativos por parte dos praticantes de Deep Learning com RNAs. Na minha opinião, as RNAs são a classe mais interessante de modelos de Aprendizado de Máquina por quatro motivos: (1) elas são <strong>extremamente simples</strong>, uma vez que tenhamos entendido os modelos lineares; (2) são <strong>bastante intuitivas,</strong> pois permitem a interpretação de aprendizado de níveis de abstrações hierárquicos; (3) são <strong>muito flexíveis</strong>, o que as torna ideais para resolver os mais diversos tipos de problemas; (4) são <strong>absurdamente efetivas</strong> quanto a qualidade dos resultados.</p>
<p>No entanto, cabe também falar sobre algumas desvantagens das redes neurais artificiais. A desvantagem mais óbvia é que os modelos baseados em RNAs são normalmente gigantescos, consumindo muita energia e recurso computacional. Em segundo lugar, treinar uma RNA é extremamente difícil, dado o formato não convexo da função custo. De fato, foi apenas recentemente (2008) que a comunidade científica conseguiu treiná-las de forma satisfatória, fazendo renascer o interesse por elas. Além disso, as RNAs estão sujeitas a sobre-ajustarem facilmente, devido a sua alta capacidade. Por conta disso, em termos práticos, quando os dados não são tão abundantes (< 10000), os resultados obtidos com RNAs não costumam ser melhores do que os obtidos com outros algoritmos de Aprendizado de Máquina.</p>

<h2 id="Referências">Referências</h2>
<ul>
	<li>Esta <a href="https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU">série de vídeos</a> explica de maneira maravilhosamente intuitiva o que são e como implementar uma RNA.</li>
	<li>Estes <a href="https://www.youtube.com/playlist?list=PLnnr1O8OWc6ZGLIFmwU97x4ts1dJrffwA">vídeos da Universidade de Stanford</a> explicam de maneira mais detalhada a representação das RNAs.</li>
	<li>A Universidade de Toronto tem um <a href="https://www.youtube.com/user/aicourses/playlists?view=50&sort=dd&shelf_id=2">curso inteiro</a> sobre RNAs em Aprendizado de Máquina, ensinado por Geoffrey Hinton, um dos mais renomados pesquisadores na área de RNAs.</li>
	<li>Na verdade, as RNAs são tão populares que existem incontáveis cursos <em>online </em>excelentes sobre o assunto: [<a href="https://www.youtube.com/watch?v=SGZ6BttHMPw&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">1</a>], [<a href="https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu">2</a>], [<a href="https://br.udacity.com/course/deep-learning--ud730/">3</a>], [<a href="https://www.youtube.com/watch?v=L_6idb3ZXB0&list=PLAwxTw4SYaPl0N6-e1GvyLp5-MUMUjOKo&index=53">4</a>]</li>
	<li>Se você gosta mais de livros, <a href="http://www.deeplearningbook.org/">este aqui</a> é de onde tiro a maioria das minhas referências.</li>
</ul>

***

<ul class="pager">
  <li class="previous"><a href="/2017/03/04/bernoulli-bandits-thompson/">Anterior</a></li>
  <li class="next"><a href="https://matheusfacure.github.io/2017/03/10/backprop/">Próximo</a></li>
</ul>
